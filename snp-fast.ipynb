{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ca97a55-1ca3-48b1-ab60-92fd33752709",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## ОБЩИЙ ДЛЯ ВСЕХ РЕШЕНИЙ КОД\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "872289e9-b5cf-4c77-9ef4-647a949adb17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# НАСТРОЙКА JUPYTER LAB И ПОДКЛЮЧЕНИЕ БИБЛИОТЕК\n",
    "\n",
    "# Импортировать библиотеки: numpy, pandas pandasql для работы с датасетами\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import pandasql as ps\n",
    "\n",
    "# Импортировать библиотеку datetime для работы с датой, временем\n",
    "import datetime as dt\n",
    "\n",
    "# Импортировать библиотеки pathlib и csv для работы с файлами и импорта csv\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "# Импортировать библиотеки hashlib для работы с криптофункциями в том числе с 'md5'\n",
    "import hashlib as hs\n",
    "\n",
    "# Настройка среды Jupyter Lab\n",
    "pd.options.display.max_rows = 400\n",
    "\n",
    "# БЛОК ИМПОРТА ИСХОДНЫХ ВЕСОВЫХ ДАННЫХ\n",
    "\n",
    "    # загружаем исходные данные из файла с записями с весовыми показателями и номерами вагонов;\n",
    "    # чистим загруженные данные;\n",
    "    # генерируем и добавляем производные данные о массе;\n",
    "    # на выходе получаем таблицу 'scales'\n",
    "# Подготовить путь к исходным CSV файлам\n",
    "dir_path = Path.cwd()\n",
    "\n",
    "# Загрузить данные из файла *scales*, файл содержит сырые данные по измеренным показателям массы\n",
    "# номер состава, номер вагона, дата/время, и ряд друхих\n",
    "\n",
    "\n",
    "# Подготовить переменную с именем импортируемого CSV файла с массами цистерн\n",
    "file_name_scales = '01__snp-scales.csv'\n",
    "path_file_scales = Path(dir_path,'res', 'data', file_name_scales)\n",
    "\n",
    "# Загрузить таблицу с данными взвешивания\n",
    "with open(path_file_scales, \"r\", encoding='utf-8') as csv_file_scales:\n",
    "    scales = pd.read_csv(csv_file_scales,\n",
    "                         delimiter=';',\n",
    "                         header=0,\n",
    "                         names=  ['trnum', 'num', 'bdatetime', 'invnum', 'tcalibr', 'tare', 'brutto', 'netto', 'velocity'] )\n",
    "\n",
    "# Добавить новый индексный столбец 'id_scales'\n",
    "scales['id_scales'] = np.arange(2, len(scales['invnum'])+2, 1)\n",
    "\n",
    "# Зафиксировать количество записей полученных из CSV файла\n",
    "len_scales = len(scales)\n",
    "\n",
    "# Привести даты к типу datetime\n",
    "scales['bdatetime'] = pd.to_datetime(scales['bdatetime'], dayfirst=True, format='%d.%m.%Y %H:%M:%S')\n",
    "\n",
    "# ЧИСТИМ ДАННЫЕ\n",
    "# Удалить не нужные столбцы: 'tcalibr', 'netto1', 'velocity'\n",
    "scales.drop(['tcalibr', 'velocity'], axis=1, inplace=True)\n",
    "\n",
    "# Удалить строки у которых значение столбца invnum = NaN\n",
    "scales.dropna(subset=['invnum'], inplace=True)\n",
    "\n",
    "# Заменить тип столбца 'invnum' на int64\n",
    "scales['invnum'] = scales['invnum'].astype('int64')\n",
    "\n",
    "# ГЕНЕРИРОВАТЬ И ДОБАВИТЬ ПРОИЗВОДНЫЕ ДАННЫЕ\n",
    "# Добавить новый столбец 'deltaweight' содержащий разницу массы на въезде и на выезде\n",
    "scales['deltaweight'] = scales['brutto'] - scales['tare']\n",
    "\n",
    "# Задать новый порядок столбцов и проиндексировать\n",
    "scales=scales.reindex(columns=['id_scales', 'trnum', 'invnum', 'num', 'bdatetime', 'tare', 'brutto', 'netto', 'deltaweight'])\n",
    "\n",
    "# РАЗБИТЬ ПОЛУЧЕННЫЕ ДАННЫЕ ДВЕ ОСНОВНЫЕ РАБОЧИЕ ТАБЛИЦЫ\n",
    "\n",
    "    # выделить набор scales_in - записи с составами ВЪЕХАВШИМИ на базу;\n",
    "    # выделить набор scales_out - записи с составами ВЫЕХАВШИМИ с базы;\n",
    "    # разделение провести по значению '0' в поле 'tare'\n",
    "    # на выходе получаем таблицы 'scales_in' и 'scales_out'\n",
    "\n",
    "# ВЫделить в отдельную таблицу 'scales_in' записи с транзакцией заехавших составов, по условию 'tare' = 0\n",
    "scales_in = scales[ scales['tare'] == 0 ]\n",
    "# ВЫделить в отдельную таблицу 'scales_out' записи с транзакцией выехавших составов, по условию 'tare' != 0\n",
    "scales_out = scales[ scales['tare'] != 0 ]\n",
    "\n",
    "# Задать индексацию по 'id_scales'\n",
    "#scales_in = scales_in.set_index(keys = ['id_scales'], drop=False)\n",
    "#scales_out = scales_out.set_index(keys = ['id_scales'], drop=False)\n",
    "\n",
    "# БЛОК ИМПОРТА ИСХОДНЫХ ДАННЫХ ПО ОТБРАКОВКЕ\n",
    "\n",
    "    # загружаем исходные данные из файла с записями о забракованных вагонов;\n",
    "    # генерируем и добавляем поле 'id_marriag', которое позволит сопоставлять записи результатов работы программы с исходными 'грязными' данными до обработки\n",
    "    # чистим загруженные данные;\n",
    "    # на выходе получаем таблицу 'marriag'\n",
    "\n",
    "# Загрузить данные из файла *marriag*, файл содержит сырые данные по отбракованным вагонам\n",
    "# В итоге сосздается dataset 'marriag'\n",
    "\n",
    "# Подготовить переменную с именем импортируемого CSV файла с дефектными цистернами\n",
    "file_name_marriag = '02__snp-marriag.csv'\n",
    "path_file_marriag = Path(dir_path,'res', 'data', file_name_marriag)\n",
    "\n",
    "# Загрузить таблицу с данными отбраковки\n",
    "with open(path_file_marriag, \"r\", encoding='utf-8') as csv_file_marriag:\n",
    "    marriag = pd.read_csv(csv_file_marriag,\n",
    "                          delimiter=';',\n",
    "                          header=0,\n",
    "                          names=  ['n', 'invnum', 'tcalibr', 'mass', 'nact', 'bdatetime', 'reason', 'contractor'])\n",
    "\n",
    "# Добавить индексный столбец ID с индексом\n",
    "marriag['id_marriag'] = np.arange(2, len(marriag['invnum'])+2, 1)\n",
    "\n",
    "# Удалить не нужные столбцы\n",
    "marriag.drop(['tcalibr', 'mass'], axis=1, inplace=True)\n",
    "\n",
    "# Привести даты к типу datetime\n",
    "marriag['bdatetime'] = pd.to_datetime(marriag['bdatetime'], dayfirst=True, format='%d.%m.%Y')\n",
    "\n",
    "# Задать новый порядок столбцов и проиндексировать\n",
    "marriag = marriag.reindex(columns=['invnum', 'id_marriag',  'bdatetime', 'n', 'nact', 'reason', 'contractor'])\n",
    "\n",
    "# Задать индексацию по 'id_marriag'\n",
    "marriag = marriag.set_index(keys = ['id_marriag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af7fed7-4468-4da9-b0f4-3ad72bc81357",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## ВЕТКА 1  \n",
    "---\n",
    "РЕШЕНИЕ ЗАДАЧИ ЧЕРЕЗ ЯВНО СУЩЕСТВУЮЩИЕ ПРИЗНАКИ. НЕ ВВОДИТСЯ И НЕ ИСПОЛЬЗУЕТСЯ ПОНЯТИЕ HASH ЗНАЧЕНИЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce9b5207-d8c3-4857-9273-0b7f1300ee82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# БЛОК СОЕДИНЕНИЯ ЗАПИСЕЙ 'scales_in' И 'scales_out'\n",
    "\n",
    "# В данном блоке производится объединение записей таблиц 'scales_in' и 'scales_out' для получения 'правильных' записей о транзакции\n",
    "# состава и всех входящих в него вагонов которые въехали на базу для налива и выехали после. Соединение производится по правилу LEFT JOIN\n",
    "# с выполнением тройного условия. Так как 'pandas' не умеет выполнять оьбъединенеия по нечеткому соответствию (а именно таким получается\n",
    "# третье условие), то для соединения была задействована дополнительная библиотека 'pandassql' которая работает поверх 'pandas' и перегоняет\n",
    "# (динамичесуки) данные в SQL базу данных SQLite, и позволяет выполнить преобразование с помощью команд SQL, а после обратно сохранить в\n",
    "# объекты 'pandas' - DataFrames.\n",
    "# Условия соединенеия подразумевают совпадения следующих полей обеих DataFrames:\n",
    "\n",
    "    # 'invnum';\n",
    "    # 'num';\n",
    "    # нечеткое совпадение временного поля 'bdatetime' (по принципу оконной функции);\n",
    "    # на выходе получаем таблицу 'scales_join';\n",
    "\n",
    "# Подготовить SQL запрос для левого соединения таблиц 'scales_in' и 'scales_out' с помощью функционала библиотеки 'pandasql'\n",
    "\n",
    "join_query_scales = '''\n",
    "    SELECT\n",
    "        l.id_scales      as id_scales\n",
    "        ,r.trnum         as trnum_in\n",
    "        ,l.trnum         as trnum_out\n",
    "        ,r.invnum        as invnum_in\n",
    "        ,l.invnum        as invnum_out\n",
    "        ,r.num           as num_in\n",
    "        ,l.num           as num_out\n",
    "        ,r.bdatetime     as date_in\n",
    "        ,l.bdatetime     as date_out\n",
    "        ,r.brutto        as tare_in\n",
    "        ,l.tare          as tare_out\n",
    "        ,l.brutto        as brutto\n",
    "        ,l.netto         as netto\n",
    "        ,l.deltaweight   as deltaweight\n",
    "    FROM scales_in as r LEFT JOIN scales_out as l\n",
    "         ON (r.invnum = l.invnum)\n",
    "            AND (r.num = l.num)\n",
    "            AND (cast(strftime('%s',r.bdatetime) as interger)\n",
    "                BETWEEN cast(strftime('%s',l.bdatetime, '-37 hours') as interger) AND cast(strftime('%s',l.bdatetime) as interger) )\n",
    "'''\n",
    "\n",
    "# Выполнить ранее подготовленный запрос средствами библиотеки 'pandasql', получить результирующую таблицу\n",
    "scales_join = ps.sqldf(join_query_scales, locals())\n",
    "\n",
    "# Привести даты к типу datetime\n",
    "scales_join['date_in'] = pd.to_datetime(scales_join['date_in'])\n",
    "scales_join['date_out'] = pd.to_datetime(scales_join['date_out'])\n",
    "\n",
    "# Вставить столбец 'deltatetime' с данными разницы времени (дени - часа - минуты - секунды) между событием 'въехал' и событием 'выехал'\n",
    "scales_join.insert(9, 'deltatetime', scales_join['date_out']-scales_join['date_in'])\n",
    "\n",
    "# Нормировать разницу к формату 00,00 (часы и доли часа)\n",
    "scales_join['deltatetime'] = (scales_join['deltatetime']/pd.Timedelta('1 hour')).round(2)\n",
    "\n",
    "# Преобразовать 'deltatetime' к string и заменить '.' на ',' т.к. импортирует с '.' асурд\n",
    "scales_join['deltatetime'] = scales_join['deltatetime'].astype(str).str.replace('.', ',', regex=False)\n",
    "scales_join['deltatetime'] = scales_join['deltatetime'].astype(str).str.replace('nan', '', regex=False)\n",
    "\n",
    "# Заменить NaN в столбцах 'trnum_out', 'invnum_out', 'num_out', 'brutto', 'netto', 'deltaweigth' на 0\n",
    "scales_join[[ 'id_scales'\n",
    "             ,'trnum_out'\n",
    "             ,'invnum_out'\n",
    "             ,'num_out'\n",
    "             ,'deltatetime'\n",
    "             ,'tare_out'\n",
    "             ,'brutto'\n",
    "             ,'netto'\n",
    "             ,'deltaweight']] = scales_join[['id_scales', 'trnum_out', 'invnum_out', 'num_out' ,'deltatetime', 'tare_out', 'brutto', 'netto', 'deltaweight']].fillna(0)\n",
    "\n",
    "# Заменить тип в столбцах 'trnum_out', 'invnum_out', 'num_out', 'brutto', 'netto', 'deltaweigth' на int64\n",
    "scales_join[[ 'id_scales'\n",
    "             ,'trnum_out'\n",
    "             ,'invnum_out'\n",
    "             ,'num_out'\n",
    "             ,'tare_out'\n",
    "             ,'brutto'\n",
    "             ,'netto'\n",
    "             ,'deltaweight']] = scales_join[['id_scales', 'trnum_out', 'invnum_out', 'num_out', 'tare_out','brutto', 'netto', 'deltaweight']].astype('int64')\n",
    "\n",
    "# БЛОК СОХРАНЕНИЯ ПРОМЕЖУТОЧНОГО ФАЙЛА CSV С ДАННЫМИ ТАБЛИЦЫ 'scales_join'\n",
    "    # на выходе получаем CSV файл\n",
    "\n",
    "# Подготовить переменную с именем экспортируемого CSV файла\n",
    "file_name_scales_join_csv = '03__result-scales-correct-branch1.csv'\n",
    "path_file_scales_join_csv = Path(dir_path,'res', 'data', file_name_scales_join_csv)\n",
    "\n",
    "file_name_scales_join_excel = '03__result-scales-correct-branch1.xlsx'\n",
    "path_file_scales_join_excel = Path(dir_path,'res', 'data', file_name_scales_join_excel)\n",
    "\n",
    "# Записать данные в таблицу\n",
    "scales_join.to_csv(str(path_file_scales_join_csv), index=False, sep=';', encoding='utf-8')\n",
    "scales_join.to_excel(str(path_file_scales_join_excel), index=False)\n",
    "\n",
    "# БЛОК СОЕДИНЕНИЯ ЗАПИСЕЙ 'scales_join' И 'marriag' ДЛЯ ПОЛУЧЕНИЯ ТАБЛИЦЫ ЯВЛЯЮЩЕЙСЯ РЕШЕНИЕМ ЗАДАЧИ\n",
    "\n",
    "# В данном блоке производится объединение записей таблиц scales_join и 'marriag' для получения итоговоЙ ТАБЛИЦЫ, которая является решением задачи.\n",
    "# Позже на ее основе будет получен итоговый выходной файл правильных' записей о транзакции состава и всех входящих в него вагонов которые въехалина\n",
    "# базу для налива и выехали после. Соединение производится по правилу LEFT JOIN с выполнением двойного условия. Так как 'pandas' не умеет выполнять\n",
    "# объединенеия по нечеткому соответствию (а именно таким получается второе условие), то для соединения была задействована дополнительная библиотека\n",
    "# 'pandassql' которая работает поверх 'pandas' и перегоняет (динамически) данные в SQL базу данных SQLite, далее позволяет выполнить преобразование\n",
    "# с помощью команд SQL, а после обратно сохранить в объекты 'pandas' - DataFrames. Условия соединенеия подразумевают совпадения следующих полей обеих таблиц:\n",
    "\n",
    "    # 'invnum_out' и 'invnum';\n",
    "    # нечеткое совпадение временного поля 'bdatetime' (по принципу оконной функции);\n",
    "    # на выходе получаем таблицу 'join_hash_table';\n",
    "\n",
    "\n",
    "# Подготовить SQL запрос для левого соединения таблиц 'scales_join' и 'marriag'\n",
    "\n",
    "join_query_result_table = '''\n",
    "    SELECT\n",
    "        ROW_NUMBER() OVER(ORDER BY l.id_scales) + 1 as id_result\n",
    "        ,l.id_scales   as id_scales\n",
    "        ,r.id_marriag  as id_marriag\n",
    "        ,l.trnum_in    as trnum_in\n",
    "        ,l.trnum_out   as trnum_out\n",
    "        ,l.invnum_in   as invnum_in\n",
    "        ,l.invnum_out  as invnum_out\n",
    "        ,l.num_in      as num_in\n",
    "        ,l.num_out     as num_out\n",
    "        ,l.date_in     as date_in\n",
    "        ,l.date_out    as date_out\n",
    "        ,r.bdatetime   as date_marriag\n",
    "        ,l.deltatetime as deltatetime\n",
    "        ,l.tare_in     as tare_in\n",
    "        ,l.tare_out    as tare_out\n",
    "        ,l.brutto      as brutto\n",
    "        ,l.netto       as netto\n",
    "        ,l.deltaweight as deltaweight\n",
    "        ,r.nact        as nact\n",
    "        ,r.reason      as reason\n",
    "    FROM scales_join as l LEFT JOIN marriag as r\n",
    "         ON (l.invnum_out = r.invnum)\n",
    "            AND (cast(strftime('%s',r.bdatetime) as interger)\n",
    "                BETWEEN cast(strftime('%s',l.date_out, '-30 hours') as interger) AND cast(strftime('%s',l.date_out) as interger) )        \n",
    "'''\n",
    "\n",
    "# Выполнить ранее подготовленный запрос средствами библиотеки 'pandasql', получить результирующую таблицу\n",
    "join_table = ps.sqldf(join_query_result_table, locals())\n",
    "\n",
    "# Привести даты к типу datetime\n",
    "join_table['date_in'] = pd.to_datetime(join_table['date_in'])\n",
    "join_table['date_out'] = pd.to_datetime(join_table['date_out'])\n",
    "join_table['date_marriag'] = pd.to_datetime(join_table['date_marriag'])\n",
    "\n",
    "# Заменить NaN в столбце 'id_marriag' на 0\n",
    "join_table['id_marriag'] = join_table['id_marriag'].fillna(0)\n",
    "\n",
    "# Заменить тип столбца 'id_marriag' на int64\n",
    "join_table['id_marriag'] = join_table['id_marriag'].astype('int64')\n",
    "\n",
    "# Заменить None в столбце 'nact' на 0\n",
    "join_table['nact'] = join_table['nact'].fillna(0)\n",
    "\n",
    "# Заменить тип столбца 'nact' на int64\n",
    "join_table['nact'] = join_table['nact'].astype('int64')\n",
    "\n",
    "\n",
    "# БЛОК СОХРАНЕНИЯ РЕШЕНИЯ ЗАДАЧИ ПО МЕТОДУ 'ВЕТКА 1' В CSV ФАЙЛ\n",
    "    # на выходе получаем CSV файл сгенерированный в соответствии с логикой 'ВЕТКА 1'\n",
    "\n",
    "# Подготовить переменную с именем результирующего CSV файла\n",
    "file_name_join = '05__result-table-branch1.csv'\n",
    "path_file_join = Path(dir_path,'res', 'data', file_name_join)\n",
    "\n",
    "file_name_join_excel = '05__result-table-branch1.xlsx'\n",
    "path_file_join_excel = Path(dir_path,'res', 'data', file_name_join_excel)\n",
    "\n",
    "# Записать в данные в файл\n",
    "join_table.to_csv(str(path_file_join), index=False, sep=';', encoding='utf-8')\n",
    "join_table.to_excel(str(path_file_join_excel), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b387fd-416a-48ab-b0b9-667e404ac5f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## ВЕТКА 2  \n",
    "---\n",
    "РЕШЕНИЕ ЗАДАЧИ ЧЕРЕЗ НЕ ЯВНО СУЩЕСТВУЮЩИЕ ПРИЗНАКИ. ДЛЯ РЕШЕНИЯ ВВОДИТСЯ И ИСПОЛЬЗУЕТСЯ ПОНЯТИЕ HASH ЗНАЧЕНИЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3e331fd-b8da-43b4-9b50-30a79a52ff98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ГРУППИРОВКА ДАННЫХ В DATAFRAMES scales_in И scales_out\n",
    "\n",
    "# В данном блоке производится группировка записей таблиц 'scales_in' и 'scales_out' для получения сгруппированных по значению номера состава\n",
    "# 'trnum' записей. Это нужно для достижения нескольких целей:\n",
    "\n",
    "    # определения количества составов отдельно в таблицах въехавших и выехавших составов;\n",
    "    # подготовка данных в обеих таблицах к такой форме хранения которая позволит эффективно вычислить hash значения;\n",
    "    # вычисления комплексных характеристик каждого состава в обеих таблицах - hash значения;\n",
    "\n",
    "# Выработка требований к hash значению - нам потребуется определить такую группу характеристик которая атомарно представляет уникальную\n",
    "# характеристику состава, причем должна имется 100% возможность получить такую группу как в таблице въехавших составах так и в таблице\n",
    "# выехавших составов. Это требование должно не укоснительно выполняться, так как в дальнейшем полученный слепок состава - hash значение\n",
    "# должно стать идентификатором по которому будет искаться совпадение и далее проводиться объединение данных по идентичным составам из обеих таблиц.\n",
    "# В качестве такого группового признака который имперически определен как абсолютно не повторяемый ни при каких условиях - последовательность\n",
    "# значений номеров состава а также (одновременно) позиционный номер вагона в составе, т.е. математическая последовательность значений номеров вагонов.\n",
    "# Для упрощения кода, фактор порядкового номера был заменен на не явный, а именно перед вычислением hash значения номера вагонов превращены в текстовые\n",
    "# значения и произведена конкатенация строго в той последовательности в которой вагоны шли в составе. В итоге входным параметром функции-генератора\n",
    "# hash значения стала тестовая строка состаящая из непрерывно сцепленных значений текстовых эквивалентов номеров вагонов.\n",
    "# В качестве функции-генератора hash значения применена - MD5\n",
    "\n",
    "\n",
    "# Вывести первые 3 записи (до группировки) комплементарных групп которе далее должны сгруппироваться а далее объединиться\n",
    "# это требуется для визуального контроля\n",
    "scales_in[ scales_in['trnum'] == 683482 ].sort_values(by='num').head(3)\n",
    "\n",
    "# Произвести группировку по номеру состава ('trnum')\n",
    "gr_scales_in = scales_in.groupby('trnum')\n",
    "gr_scales_out = scales_out.groupby('trnum')\n",
    "\n",
    "# БЛОК ГЕНЕРАЦИИ HASH ЗНАЧЕНИЙ\n",
    "\n",
    "# Функция вычисляет hash значение группы\n",
    "def HashRow(_df):\n",
    "    \n",
    "    hash_total_str = ''\n",
    "    \n",
    "    df = _df.sort_values(by='num', ascending=True)\n",
    "    # Перебрать все строки текущей группы, получить текстовый объект hash всех номеров вагонов группы\n",
    "    for data in df.itertuples():\n",
    "        hash_total_str += str(data[3])\n",
    "    \n",
    "    hash_object = hs.md5(hash_total_str.encode('utf-8'))\n",
    "    #return int(hash_object.hexdigest(), 16)\n",
    "    return hash_object.hexdigest()\n",
    "\n",
    "# Функция выпоняет перебор всех групп и вызывает функцию вычисления hash значения группы\n",
    "def HashGroup(_df):\n",
    "    \n",
    "    hash_dict = {} # Словарь хранящий пару 'Номер состава: hash состава'\n",
    "    cycle = 0\n",
    "    \n",
    "    # Перебраить все группы сгруппированого DataFrame\n",
    "    for name, group in _df:\n",
    "        if cycle>0:\n",
    "            break\n",
    "        hash_dict[name] = HashRow(group) # Для каждой группы вызвать hash функцию\n",
    "    return hash_dict\n",
    "\n",
    "# Сгенерировать словари с hash значениями групп составовs\n",
    "hash_dict_in  = HashGroup(gr_scales_in)\n",
    "hash_dict_out = HashGroup(gr_scales_out)\n",
    "\n",
    "# Преобразовать dictonari в DataFrame\n",
    "hash_df_in = pd.DataFrame(hash_dict_in.items(), columns=['trnum', 'hash_group'])\n",
    "hash_df_out = pd.DataFrame(hash_dict_out.items(), columns=['trnum', 'hash_group'])\n",
    "\n",
    "# БЛОК СОЕДИНЕНИЯ ТАБЛИЦ 'scales_in','scales_out' С СООТВЕТСТВУЮЩИМИ ТАБЛИЦАМИ HASH ЗНАЧЕНИЙ\n",
    "\n",
    "# Выполнить левое соединение таблиц 'scales_hash_in', 'scales_hash_out' с соответствующими таблицами hash значений\n",
    "scales_hash_in = scales_in.merge(hash_df_in, on='trnum', how='left')\n",
    "scales_hash_out = scales_out.merge(hash_df_out, on='trnum', how='left')\n",
    "\n",
    "# БЛОК СОЕДИНЕНИЯ ЗАПИСЕЙ 'scales_hash_in' И 'scales_hash_out' ДЛЯ ПОЛУЧЕНИЯ ТАБЛИЦЫ 'scales_join_hash'\n",
    "# КОТОРАЯ ХРАНИТ ОБЪЕДИНЕНЫЕ ЗАПИСИ ВЪЕХАВШИХ И ВЫЕХАВШИХ СОСТАВОВ.\n",
    "\n",
    "# АНАЛОГИЧНАЯ ТАБЛИЦА ПОЛУЧЕНА РАНЕЕ В ВЕТКЕ 1 ПО ОТЛИЧНОМУ ОТ ВЕТКЕ 2 АЛГОРИТМУ, А ИМЕННО БЕЗ ИСПОЛЬЗОВАНИЯ HASH ЗНАЧЕНИЙ\n",
    "\n",
    "# В данном блоке производится соединение записей таблиц 'scales_hash_in' и 'scales_hash_out'. Позже на основе результирующей\n",
    "# таблицы 'scales_join_hash' будет записан результирующий выходной файл CSV файл, являющийся решением задачи.\n",
    "# Соединение производится по правилу LEFT JOIN с выполнением тройного условия. В данном случае у нас все три условия четкие,\n",
    "# поэтому можно произвести объединение средствами библиотеки 'pandas', но из за экономии времени, единообразности технических\n",
    "# решений ранее уже примененных в данной программе, а также гибкости компоновки сстолбцов итоговой таблицы, соединение будет\n",
    "# производится средствами библиотеки 'pandassql', которая работает поверх 'pandas' и перегоняет (динамически) данные в SQL базу\n",
    "# данных SQLite, далее позволяет выполнить преобразование с помощью команд SQL, а после обратно сохранить в объекты 'pandas' - DataFrames.\n",
    "# Условия соединенеия подразумевают совпадения следующих полей обеих таблиц:\n",
    "\n",
    "    # 'invnum_out' и 'invnum';\n",
    "    # 'num' обеих таблиц;\n",
    "    # 'hash_group' обеих таблиц;\n",
    "    # на выходе получаем таблицу 'scales_join_hash';\n",
    "\n",
    "    # Преобразуем тип 'hash_group' в string, потому что pandassql не умеет конвертировать int large в целочисленные типы SQLite\n",
    "\n",
    "# Данное действие требуется только для варианта программы где 'hash' значение конвертировалось в int larg, если работа ведется \n",
    "# с текстовыми 'hash' значениями то оно не нужно, поэтому эти строки кода могут быть закоментированы\n",
    "\n",
    "#scales_hash_in['hash_group'] = scales_hash_in['hash_group'].astype(str)\n",
    "#scales_hash_out['hash_group'] = scales_hash_out['hash_group'].astype(str)\n",
    "\n",
    "# Подготовить SQL запрос для левого соединения таблиц 'scales_hash_in' и 'scales_hash_out'\n",
    "join_query_scales_hash = '''\n",
    "    SELECT\n",
    "        l.id_scales     as id_scales\n",
    "        ,l.trnum         as trnum_in\n",
    "        ,r.trnum         as trnum_out\n",
    "        ,l.invnum        as invnum_in\n",
    "        ,r.invnum        as invnum_out\n",
    "        ,l.num           as num_in\n",
    "        ,l.num           as num_out\n",
    "        ,l.bdatetime     as date_in\n",
    "        ,r.bdatetime     as date_out\n",
    "        ,l.brutto        as tare_in\n",
    "        ,r.tare          as tare_out\n",
    "        ,r.brutto        as brutto\n",
    "        ,r.netto         as netto\n",
    "        ,r.deltaweight   as deltaweight\n",
    "        ,l.hash_group    as hash_group\n",
    "    FROM scales_hash_in as l LEFT JOIN scales_hash_out as r\n",
    "         ON     (l.invnum     = r.invnum)\n",
    "            AND (l.num        = r.num)\n",
    "            AND (l.hash_group = r.hash_group)\n",
    "'''\n",
    "\n",
    "# Выполнить ранее подготовленный запрос средствами библиотеки 'pandasql', получить результирующую таблицу\n",
    "scales_join_hash = ps.sqldf(join_query_scales_hash, locals())\n",
    "\n",
    "# Привести даты к типу datetime\n",
    "scales_join_hash['date_in']  = pd.to_datetime(scales_join_hash['date_in'])\n",
    "scales_join_hash['date_out'] = pd.to_datetime(scales_join_hash['date_out'])\n",
    "\n",
    "# Вставить столбец 'deltatetime' с данными разницы времени (дени - часа - минуты - секунды) между событием 'въехал' и событием 'выехал'\n",
    "scales_join_hash.insert(9, 'deltatetime', scales_join_hash['date_out']-scales_join_hash['date_in'])\n",
    "\n",
    "# Нормировать разницу к формату 00,00 (часы и доли часа)\n",
    "scales_join_hash['deltatetime'] = (scales_join_hash['deltatetime']/pd.Timedelta('1 hour')).round(2)\n",
    "\n",
    "# Преобразовать 'deltatetime' к string и заменить '.' на ',' т.к. импортирует с '.' для дальнейшего использования совместно с Excel\n",
    "scales_join_hash['deltatetime'] = scales_join_hash['deltatetime'].astype(str).str.replace('.', ',', regex=False)\n",
    "\n",
    "# Заменить NaN в столбцах 'trnum_out', 'invnum_out', 'num_out', 'brutto', 'netto', 'deltaweigth' на 0\n",
    "scales_join_hash[['trnum_out'\n",
    "                 ,'invnum_out'\n",
    "                 ,'num_out'\n",
    "                 ,'deltatetime'\n",
    "                 ,'tare_out'\n",
    "                 ,'brutto'\n",
    "                 ,'netto'\n",
    "                 ,'deltaweight']] = scales_join_hash[['trnum_out', 'invnum_out', 'num_out' ,'deltatetime', 'tare_out', 'brutto', 'netto', 'deltaweight']].fillna(0)\n",
    "\n",
    "# Заменить тип в столбцах 'trnum_out', 'invnum_out', 'num_out', 'brutto', 'netto', 'deltaweigth' на int64\n",
    "scales_join_hash[['trnum_out'\n",
    "                 ,'invnum_out'\n",
    "                 ,'num_out'\n",
    "                 ,'tare_out'\n",
    "                 ,'brutto'\n",
    "                 ,'netto'\n",
    "                 ,'deltaweight']] = scales_join_hash[['trnum_out', 'invnum_out', 'num_out', 'tare_out','brutto', 'netto', 'deltaweight']].astype('int64')\n",
    "\n",
    "# БЛОК СОХРАНЕНИЯ ПРОМЕЖУТОЧНОГО ФАЙЛА CSV С ДАННЫМИ ТАБЛИЦЫ 'scales_join'\n",
    "    # на выходе получаем CSV файл сгенерированный в соответствии с логикой 'ВЕТКА 2'\n",
    "\n",
    "# Подготовить переменную с именем экспортируемого CSV файла \n",
    "file_name_scales_join_hash_csv = '04__result-scales-correct-branch2.csv'\n",
    "path_file_scales_join_hash_csv = Path(dir_path,'res', 'data', file_name_scales_join_hash_csv)\n",
    "\n",
    "file_name_scales_join_hash_excel = '04__result-scales-correct-branch2.xlsx'\n",
    "path_file_scales_join_hash_excel = Path(dir_path,'res', 'data', file_name_scales_join_hash_excel)\n",
    "\n",
    "# Записать в данные в таблицу\n",
    "scales_join_hash.to_csv(str(path_file_scales_join_hash_csv), index=False, sep=';', encoding='utf-8')\n",
    "scales_join_hash.to_excel(str(path_file_scales_join_hash_excel), index=False)\n",
    "\n",
    "# БЛОК СОЕДИНЕНИЯ ЗАПИСЕЙ 'scales_join_hash' И 'marriag' ДЛЯ ПОЛУЧЕНИЯ ТАБЛИЦЫ ЯВЛЯЮЩЕЙСЯ РЕШЕНИЕМ ЗАДАЧИ\n",
    "# В данном блоке производится объединение записей таблиц scales_join_hash и 'marriag' для получения итоговоЙ ТАБЛИЦЫ,\n",
    "# которая является решением задачи. Позже на ее основе будет получен итоговый выходной файл правильных' записей о транзакции\n",
    "# состава и всех входящих в него вагонов которые въехали на базу для налива и выехали после. Соединение производится по правилу\n",
    "# LEFT JOIN с выполнением двойного условия. Так как 'pandas' не умеет выполнять оьбъединенеия по нечеткому соответствию (а именно\n",
    "# таким получается второе условие), то для соединения была задействована дополнительная библиотека 'pandassql' которая работает\n",
    "# поверх 'pandas' и перегоняет (динамически) данные в SQL базу данных SQLite, далее позволяет выполнить преобразование с помощью\n",
    "# команд SQL, а после обратно сохранить в объекты 'pandas' - DataFrames. Условия соединенеия подразумевают совпадения следующих\n",
    "# полей обеих таблиц:\n",
    "\n",
    "    # 'invnum_out' и 'invnum';\n",
    "    # нечеткое совпадение временного поля 'bdatetime' (по принципу оконной функции);\n",
    "    # на выходе получаем таблицу 'join_table';\n",
    "\n",
    "# Подготовить SQL запрос для левого соединения таблиц 'scales_join_hash' и 'marriag'\n",
    "\n",
    "join_hash_query_result_table = '''\n",
    "    SELECT\n",
    "        ROW_NUMBER() OVER(ORDER BY l.id_scales) + 1 as id_result\n",
    "        ,l.id_scales   as id_scales\n",
    "        ,r.id_marriag  as id_marriag\n",
    "        ,l.trnum_in    as trnum_in\n",
    "        ,l.trnum_out   as trnum_out\n",
    "        ,l.invnum_in   as invnum_in\n",
    "        ,l.invnum_out  as invnum_out\n",
    "        ,l.num_in      as num_in\n",
    "        ,l.num_out     as num_out\n",
    "        ,l.date_in     as date_in\n",
    "        ,l.date_out    as date_out\n",
    "        ,r.bdatetime   as date_marriag\n",
    "        ,l.deltatetime as deltatetime\n",
    "        ,l.tare_in     as tare_in\n",
    "        ,l.tare_out    as tare_out\n",
    "        ,l.brutto      as brutto\n",
    "        ,l.netto       as netto\n",
    "        ,l.deltaweight as deltaweight\n",
    "        ,r.nact        as nact\n",
    "        ,r.reason      as reason\n",
    "        ,l.hash_group  as hash_group\n",
    "    FROM scales_join_hash as l LEFT JOIN marriag as r\n",
    "         ON (l.invnum_out = r.invnum)\n",
    "            AND (cast(strftime('%s',r.bdatetime) as interger)\n",
    "                BETWEEN cast(strftime('%s',l.date_out, '-30 hours') as interger) AND cast(strftime('%s',l.date_out) as interger) )        \n",
    "'''\n",
    "\n",
    "# Выполнить ранее подготовленный запрос средствами библиотеки 'pandasql', получить результирующую таблицу\n",
    "join_hash_table = ps.sqldf(join_hash_query_result_table, locals())\n",
    "\n",
    "# Привести даты к типу datetime\n",
    "join_hash_table['date_in'] = pd.to_datetime(join_hash_table['date_in'])\n",
    "join_hash_table['date_out'] = pd.to_datetime(join_hash_table['date_out'])\n",
    "join_hash_table['date_marriag'] = pd.to_datetime(join_hash_table['date_marriag'])\n",
    "\n",
    "# Заменить NaN в столбце 'id_marriag' на 0\n",
    "join_hash_table['id_marriag'] = join_hash_table['id_marriag'].fillna(0)\n",
    "\n",
    "# Заменить тип столбца 'id_marriag' на int64\n",
    "join_hash_table['id_marriag'] = join_hash_table['id_marriag'].astype('int64')\n",
    "\n",
    "# Заменить None в столбце 'nact' на 0\n",
    "join_hash_table['nact'] = join_hash_table['nact'].fillna(0)\n",
    "\n",
    "# Заменить тип столбца 'nact' на int64\n",
    "join_hash_table['nact'] = join_hash_table['nact'].astype('int64')\n",
    "\n",
    "\n",
    "# БЛОК СОХРАНЕНИЯ РЕШЕНИЯ ЗАДАЧИ ПО МЕТОДУ 'ВЕТКА 2' В CSV ФАЙЛ\n",
    "\n",
    "# Подготовить переменную с именем результирующего CSV файла\n",
    "file_name_join_hash_table_csv = '06__result-table-branch2.csv'\n",
    "path_file_join_hash_table_csv = Path(dir_path,'res', 'data', file_name_join_hash_table_csv)\n",
    "\n",
    "file_name_join_hash_table_excel = '06__result-table-branch2.xlsx'\n",
    "path_file_join_hash_table_excel = Path(dir_path,'res', 'data', file_name_join_hash_table_excel)\n",
    "\n",
    "# Записать в данные в файл\n",
    "join_hash_table.to_csv(str(path_file_join_hash_table_csv), index=False, sep=';', encoding='utf-8')\n",
    "join_hash_table.to_excel(str(path_file_join_hash_table_excel), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad576a108e8407fa612c462da2b922c446627f16b36f7d316a57b8c93f3eb233"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
